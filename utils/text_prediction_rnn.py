# -*- coding: utf-8 -*-
"""
7_Text_Prediction_RNN.py

Automatically generated by Colab and updated for readability and formatting.
"""

# Import required libraries
import os
import re
import contractions
import emoji
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import pickle
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Disable TensorFlow's OneDNN optimizations for compatibility
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

# Load pre-trained model and tokenizer
try:
    model = load_model('models/rnn_model.keras')  # Ensure the correct path to the model
    with open('models/tokenizer_rnn.pkl', 'rb') as f:
        tokenizer = pickle.load(f)
except Exception as e:
    print(f"Error loading model or tokenizer: {e}")
    exit()

# Initialize stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    """
    Preprocesses the input text by cleaning and tokenizing it.
    """
    text = text.lower()  # Lowercase
    text = contractions.fix(text)  # Expand contractions
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)  # Remove URLs
    text = re.sub(r'\@\w+|\#', '', text)  # Remove mentions and hashtags
    text = emoji.demojize(text)  # Convert emojis to text
    text = re.sub(r'_', ' ', text)  # Remove underscores from emoji descriptions
    text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
    text = re.sub(r'\d+', '', text)  # Remove digits
    text = word_tokenize(text)  # Tokenize
    text = ' '.join([word for word in text if word not in stop_words])  # Remove stopwords
    text = ' '.join([lemmatizer.lemmatize(word) for word in text])  # Lemmatize
    return text

def predict_cyberbullying(text):
    """
    Predicts whether the input text is cyberbullying or not using the RNN model.
    """
    # Preprocess the input text
    cleaned_text = preprocess_text(text)

    # Convert the cleaned text into sequences using the tokenizer
    text_sequence = tokenizer.texts_to_sequences([cleaned_text])

    # Pad the sequences to match the input shape of the model
    padded_text = pad_sequences(text_sequence, maxlen=100, padding='pre', truncating='post')

    # Predict using the model
    prediction = model.predict(padded_text)

    # Return the prediction result
    return "Cyberbullying" if prediction[0] > 0.5 else "Not Cyberbullying"

if __name__ == "__main__":
    # Get user input for prediction
    user_input = input("Enter the comment: ")
    result = predict_cyberbullying(user_input)
    print(f"Prediction: {result}")
